# [noctrex/MiniMax-M2-139B]
# model = /models/huggingface/hub/models--noctrex--MiniMax-M2-REAP-139B-A10B-MXFP4_MOE-GGUF/snapshots/cc06d93e4cb80d0675be2305dcdb545d1e42d6cc/MiniMax-M2-REAP-139B-A10B-MXFP4_MOE-00001-of-00005.gguf
# unfortunatelly it doesn't fit to 128G RAM for 192k context ;( 
# ctx-size = 196608
# ctx-size = 167936

# look https://unsloth.ai/docs/models/qwen3.5
[unsloth/Qwen3.5-120B]
model=/models/huggingface/hub/models--unsloth--Qwen3.5-122B-A10B-GGUF/snapshots/d503adb1b5f3b29521713c7460735ed753b04e68/MXFP4_MOE/Qwen3.5-122B-A10B-MXFP4_MOE-00001-of-00003.gguf
ctx-size=262144
temp=0.6
top-p=0.95
top-k=20
min-p=0.00

[Qwen/Qwen3-Coder-Next-80B-Q8]
model = /models/huggingface/hub/models--Qwen--Qwen3-Coder-Next-GGUF/snapshots/b82fb7382639d97b38fa7672e526c760c2fb358e/Qwen3-Coder-Next-Q8_0/Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf
ctx-size=262144 
cont-batching=true
temp=1.0
top-p=0.95
top-k=40

[rushyrush/MiniMax-M2.1-139B]
model = /models/huggingface/hub/models--rushyrush-MiniMax-M2.1-REAP-139B-A10B-GGUF/snapshots/
ctx-size = 167936

[noctrex/Qwen3-Next-80B]
model = /models/huggingface/hub/models--noctrex--Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-GGUF/snapshots/4c791a92568d68271a1f38403b7932a518631787/Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-00001-of-00003.gguf
ctx-size = 1048576
rope-scaling = yarn 
rope-scale = 4


[unsloth/GLM-4.7-Flash-30B]
model = /models/huggingface/hub/models--unsloth--GLM-4.7-Flash-GGUF/snapshots/66205ea11ef638d83850389bd921e6c199f7504e/GLM-4.7-Flash-Q8_0.gguf
temp = 0.7 
top-p = 1.0
min-p = 0.01
 
[endyjasmi/Qwen3-Embedding-8B]
model = /models/huggingface/hub/models--endyjasmi--Qwen3-Embedding-8B-Q4_K_M-GGUF/snapshots/b74bf2a75d37d23867612c1b81655e4496535086/qwen3-embedding-8b-q4_k_m.gguf

# stupid
# [cturan/IQuest-Coder-V1-40B]
# model = /models/huggingface/hub/models--cturan--IQuest-Coder-V1-40B-Instruct-GGUF/snapshots/c520c173b24f57d25e6431598cd2b3d2c4c8748e/IQuest-Coder-V1-40B-Instruct.Q4_K_M.gguf

# stupid
# [tiiuae/Falcon-H1R-7B]
# model = /models/huggingface/hub/models--tiiuae--Falcon-H1R-7B-GGUF/snapshots/21539e5e8334bfd5474da7fffd0f82d0a97c6d0e/Falcon-H1R-7B-Q8_0.gguf

# too stupid
# [noctrex/HyperNova-60B]
# model = /models/huggingface/hub/models--noctrex--HyperNova-60B-MXFP4_MOE-GGUF/snapshots/81f93ea3f69d38c89c8e032766573b5962185a73/HyperNova-60B-MXFP4_MOE.gguf 

# too old
# [noctrex/Nemotron-3-Nano-30B]
# model = /models/huggingface/hub/models--noctrex--Nemotron-3-Nano-30B-A3B-MXFP4_MOE-GGUF/snapshots/175835a96514b1c3c12a5f4b842c829d26dba202/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf

# too slow, but works
# [unsloth/Devstral-2-123B-Instruct-2512-GGUF]
# model = /models/huggingface/hub/models--unsloth--Devstral-2-123B-Instruct-2512-GGUF/snapshots/1f2bfbe35f7f9071d9b318374bf5eeffefab4459/UD-Q4_K_XL/Devstral-2-123B-Instruct-2512-UD-Q4_K_XL-00001-of-00002.gguf

# [bartowski/zai-org_GLM-4.7]
# model = /models/huggingface/hub/models--bartowski--zai-org_GLM-4.7-GGUF/snapshots/c47ba4573051e4a5146879669a8a95743b5302a4/zai-org_GLM-4.7-IQ1_M/zai-org_GLM-4.7-IQ1_M-00001-of-00003.gguf


