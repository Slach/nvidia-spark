[noctrex/MiniMax-M2-139B]
model = /models/huggingface/hub/models--noctrex--MiniMax-M2-REAP-139B-A10B-MXFP4_MOE-GGUF/snapshots/cc06d93e4cb80d0675be2305dcdb545d1e42d6cc/MiniMax-M2-REAP-139B-A10B-MXFP4_MOE-00001-of-00005.gguf
# unfortunatelly it doesn't fit to 128G RAM for 192k context ;( 
# ctx-size = 196608
ctx-size = 167936


[noctrex/Qwen3-Next-80B]
model = /models/huggingface/hub/models--noctrex--Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-GGUF/snapshots/4c791a92568d68271a1f38403b7932a518631787/Qwen3-Next-80B-A3B-Instruct-1M-MXFP4_MOE-00001-of-00003.gguf
ctx-size = 1048576
rope-scaling = yarn 
rope-scale = 4

[noctrex/Nemotron-3-Nano-30B]
model = /models/huggingface/hub/models--noctrex--Nemotron-3-Nano-30B-A3B-MXFP4_MOE-GGUF/snapshots/175835a96514b1c3c12a5f4b842c829d26dba202/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf

# too slow, but works
# [unsloth/Devstral-2-123B-Instruct-2512-GGUF]
# model = /models/huggingface/hub/models--unsloth--Devstral-2-123B-Instruct-2512-GGUF/snapshots/1f2bfbe35f7f9071d9b318374bf5eeffefab4459/UD-Q4_K_XL/Devstral-2-123B-Instruct-2512-UD-Q4_K_XL-00001-of-00002.gguf

[bartowski/zai-org_GLM-4.7]
model = /models/huggingface/hub/models--bartowski--zai-org_GLM-4.7-GGUF/snapshots/c47ba4573051e4a5146879669a8a95743b5302a4/zai-org_GLM-4.7-IQ1_M/zai-org_GLM-4.7-IQ1_M-00001-of-00003.gguf

[endyjasmi/Qwen3-Embedding-8B]
model = /models/huggingface/hub/models--endyjasmi--Qwen3-Embedding-8B-Q4_K_M-GGUF/snapshots/b74bf2a75d37d23867612c1b81655e4496535086/qwen3-embedding-8b-q4_k_m.gguf

[cturan/IQuest-Coder-V1-40B-Instruct-GGUF]
model = /models/huggingface/hub/models--cturan--IQuest-Coder-V1-40B-Instruct-GGUF/snapshots/c520c173b24f57d25e6431598cd2b3d2c4c8748e/IQuest-Coder-V1-40B-Instruct.Q4_K_M.gguf
