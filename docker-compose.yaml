services:
  llama.cpp:
    build:
      context: .
      dockerfile: Dockerfile.llama.cpp.spark
    image: "llama.cpp:spark-full"
    volumes:
      - ${HOME}/.cache/huggingface:/models/huggingface:ro
      - ./llama.cpp.models.ini:/etc/llama.cpp/config.ini
    entrypoint: /app/llama-server
    command:
      - --models-max
      - "1"
      - --models-preset
      - /etc/llama.cpp/config.ini
      - --host
      - 0.0.0.0
      - --port
      - "8090"
    ports:
      - "127.0.0.1:8090:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 10s

  vibe-kanban:
    build:
      context: .
      dockerfile: Dockerfile.vibe-kanban
    image: vibe-kanban:spark
    depends_on:
      - llama.cpp
    volumes:
      - ${HOME}:/root/
    ports:
      - "8888:8888"
    networks:
      - spark-network
    environment:
      - DISPLAY=${DISPLAY}
      - HOME=/root
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  spark-network:
    driver: bridge
    