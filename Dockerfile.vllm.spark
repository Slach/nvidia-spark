# The vLLM Dockerfile is used to construct vLLM image against torch nightly that can be directly used for testing

ARG CUDA_VERSION=13.0.2
ARG CUDA_VERSION_SHORT=13.0
ARG PYTHON_VERSION=3.12
ARG UBUNTU_VERSION=24.04
ARG NUMBA_VERSION=0.61.2
ARG TORCH_CUDA_ARCH_LIST='12.1'
ARG VLLM_RELEASE=0.15.0
ARG PYTORCH_URL=https://download.pytorch.org/whl/nightly/cu130


#################### BASE BUILD IMAGE ####################
# prepare basic build environment
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS vllm-base
ARG CUDA_VERSION
ARG CUDA_VERSION_SHORT
ARG PYTHON_VERSION
ARG PYTORCH_URL
ARG NUMBA_VERSION
ARG TARGETPLATFORM
ARG VLLM_RELEASE
ENV DEBIAN_FRONTEND=noninteractive
# Install Python and other dependencies
RUN apt-get update -y && \
    apt-get install -y ccache software-properties-common git curl python3 python3-dev python3-venv python3-pip pipx
ENV PATH="/root/.local/bin:${PATH}"

# Install uv for faster pip installs
RUN --mount=type=cache,target=/root/.cache/uv \
    pipx install uv

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-${CUDA_VERSION_SHORT}/compat/

RUN export CUDA_HOME=/usr/local/cuda-${CUDA_VERSION_SHORT}/compat/ && \
    uv pip install --system --break-system-packages setuptools packaging && \
    uv pip install vllm==${VLLM_RELEASE} --system --break-system-packages --extra-index-url https://wheels.vllm.ai/cu130 --extra-index-url https://download.pytorch.org/whl/cu130
