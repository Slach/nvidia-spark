# The vLLM Dockerfile is used to construct vLLM image against torch nightly that can be directly used for testing

ARG CUDA_VERSION=13.0.2
ARG PYTHON_VERSION=3.14
ARG UBUNTU_VERSION=24.04
ARG NUMBA_VERSION=0.61.2
ARG TORCH_CUDA_ARCH_LIST='12.1'
ARG PYTORCH_URL=https://download.pytorch.org/whl/nightly/cu130


#################### BASE BUILD IMAGE ####################
# prepare basic build environment
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS base
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG PYTORCH_URL
ARG NUMBA_VERSION
ARG TARGETPLATFORM
ENV DEBIAN_FRONTEND=noninteractive
# Install Python and other dependencies
RUN apt-get update -y && \
    apt-get install -y ccache software-properties-common git curl python3 python3-dev python3-venv python3-pip pipx
ENV PATH="/root/.local/bin:${PATH}"

# Install uv for faster pip installs
RUN --mount=type=cache,target=/root/.cache/uv \
    pipx install uv

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500


# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

WORKDIR /workspace

# install build and runtime dependencies
COPY requirements/common.txt requirements/common.txt
COPY use_existing_torch.py use_existing_torch.py
COPY pyproject.toml pyproject.toml

# install build and runtime dependencies without stable torch version
RUN python3 use_existing_torch.py

# install torch nightly
ARG PINNED_TORCH_VERSION
RUN --mount=type=cache,target=/root/.cache/uv \
    if [ -n "$PINNED_TORCH_VERSION" ]; then \
      pkgs="$PINNED_TORCH_VERSION"; \
    else \
      pkgs="torch torchaudio torchvision"; \
    fi && \
    uv pip install --break-system-packages --system $pkgs --index-url ${PYTORCH_URL}

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system numba==${NUMBA_VERSION}

RUN --mount=type=cache,target=/root/.cache/uv \
uv pip install --break-system-packages --system -r requirements/common.txt

# build can take a long time, and the torch nightly version fetched from url can be different in next docker stage.
# track the nightly torch version used in the build, when we set up runtime environment we can make sure the version is the same
RUN uv pip freeze | grep -i '^torch\|^torchvision\|^torchaudio' > torch_build_versions.txt
RUN cat torch_build_versions.txt

# cuda arch list used by torch
# can be useful for `test`
# explicitly set the list to avoid issues with torch 2.2
# see https://github.com/pytorch/pytorch/pull/123243

#################### BASE BUILD IMAGE ####################

#################### WHEEL BUILD IMAGE ####################
FROM base AS build
ARG TARGETPLATFORM
ARG TORCH_CUDA_ARCH_LIST
ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500

COPY . .

RUN python3 use_existing_torch.py

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system -r requirements/build.txt

ARG GIT_REPO_CHECK=0
RUN --mount=type=bind,source=.git,target=.git \
    if [ "$GIT_REPO_CHECK" != "0" ]; then bash tools/check_repo.sh ; fi

# Max jobs used by Ninja to build extensions
ARG max_jobs=20
ENV MAX_JOBS=${max_jobs}
ARG nvcc_threads=2
ENV NVCC_THREADS=$nvcc_threads

ENV CCACHE_DIR=/root/.cache/ccache
RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=.git,target=.git  \
    # Clean any existing CMake artifacts
    rm -rf .deps && \
    mkdir -p .deps && \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp314;

#################### WHEEL BUILD IMAGE ####################

################### VLLM INSTALLED IMAGE ####################
# Setup clean environment for vLLM and its dependencies for test and api server using ubuntu${UBUNTU_VERSION} with AOT flashinfer
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS vllm-base

# prepare for environment starts
ARG TARGETPLATFORM
ARG PYTHON_VERSION
ARG PYTORCH_URL

WORKDIR /vllm-workspace
ENV DEBIAN_FRONTEND=noninteractive

RUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\.//g') && \
    echo "export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}" >> /etc/environment

# Install Python and other dependencies
RUN apt-get update -y \
    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip python3-venv pipx jq \
    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 

ENV PATH="/root/.local/bin:${PATH}"


RUN --mount=type=cache,target=/root/.cache/uv \
    pipx install uv

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

# get the nightly torch version used in the build to make sure the version is the same
COPY --from=base /workspace/torch_build_versions.txt ./torch_build_versions.txt

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system $(cat torch_build_versions.txt | xargs) --index-url ${PYTORCH_URL}

# install the vllm wheel
RUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/vllm-dist \
    --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system vllm-dist/*.whl --verbose

# install package for build flashinfer
# see issue: https://github.com/flashinfer-ai/flashinfer/issues/738
RUN uv pip install --break-system-packages --system setuptools packaging ninja build


# build flashinfer for torch nightly from source around 10 mins
# @todo: cache flashinfer build result for faster build
ENV CCACHE_DIR=/root/.cache/ccache
RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/uv \
    echo "git clone flashinfer..." \
    && git clone --recursive https://github.com/flashinfer-ai/flashinfer.git \
    && cd flashinfer \
    && git checkout $(curl -sL -H "Accept: application/json" https://github.com/flashinfer-ai/flashinfer/releases/latest | jq -c -r .tag_name) \
    && git submodule update --init --recursive \
    && echo "finish git clone flashinfer..." \
    && rm -rf build \
    && export TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST} \
    && FLASHINFER_ENABLE_AOT=1 python3 setup.py bdist_wheel --dist-dir=../flashinfer-dist --verbose \
    && cd .. \
    && rm -rf flashinfer

# install flashinfer
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system flashinfer-dist/*.whl --verbose

# install common packages
COPY requirements/common.txt requirements/common.txt
COPY use_existing_torch.py use_existing_torch.py
COPY pyproject.toml pyproject.toml

COPY examples examples
COPY benchmarks benchmarks
COPY ./vllm/collect_env.py .

RUN python3 use_existing_torch.py
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system -r requirements/common.txt

################### VLLM INSTALLED IMAGE ####################


#################### UNITTEST IMAGE #############################
FROM vllm-base AS test
COPY tests/ tests/

# install build and runtime dependencies without stable torch version
COPY requirements/nightly_torch_test.txt requirements/nightly_torch_test.txt

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500

# install development dependencies (for testing)
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system -e tests/vllm_test_utils

# enable fast downloads from hf (for testing)
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system hf_transfer
ENV HF_HUB_ENABLE_HF_TRANSFER 1

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --break-system-packages --system -r requirements/nightly_torch_test.txt

# Logging to confirm the torch versions
RUN pip freeze | grep -E 'torch|vllm|flashinfer'

# Logging to confirm all the packages are installed
RUN pip freeze

#################### UNITTEST IMAGE #############################
